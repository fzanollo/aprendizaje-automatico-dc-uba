{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_06_metricas-published.ipynb)\n","\n","# Evaluación"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from IPython.display import display\n"]},{"cell_type":"markdown","metadata":{},"source":["## Matrices de confusión\n","\n","Recordemos que una matriz de confusión nos permite observar el resultado de nuestra clasificación. Para armarla:\n","- cada fila los valores observados o reales\n","- cada columna los valores predichos\n","\n","Y en cada celda:\n","- $m_{i,i}$ las instancias bien clasificadas\n","- $m_{i,j}\\ (con\\ i\\neq j)$ las instancias mal clasificadas (era de instancia $i$ pero el clasificador dijo $j$)\n","\n","Implementar la siguiente función para poder construir una matriz de confusión binaria. Deberá tomar la etiqueta que es considerada \"éxito\" como parámetro."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Tuple, Any\n","\n","def confusion_matrix(y_real: list, y_predicted: list, positive_label: Any, show: bool =False) -> Tuple[int, int, int, int]:\n","    # Construye una matriz de confusión (binaria)\n","    # y_actual es la secuencia de etiquetas reales\n","    # y_predicted es la secuencia de etiquetas predichas por el clasificador\n","    # positive_label indica cuál es la etiqueta considerada positiva.\n","\n","    tp = \"COMPLETAR\"  # verdaderos positivos\n","    tn = \"COMPLETAR\"  # verdaderos negativos\n","    fp = \"COMPLETAR\"  # falsos positivos\n","    fn = \"COMPLETAR\"  # falsos negativos\n","    \n","    if show:\n","        display(pd.DataFrame([[tp, fn], [fp, tn]], index=[\"real +\", \"real -\"], columns=[\"pred +\", \"pred -\"]))\n","        \n","        \n","    return tp, tn, fp, fn\n"]},{"cell_type":"markdown","metadata":{},"source":["### Test 1"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a probar ahora la función. Sabiendo que se recibieron 990 emails. Y que se recibieron, en este orden:\n","  - 10 correos no deseados\n","  - 978 correos \n","  - 2 correos no deseados\n","  \n","El filtro anti-spam estableció las siguientes clasificaciones (también en órden):\n","  - 2 correos no deseados\n","  - 900 correos\n","  - 20 correos no deseados\n","  - 68 correos\n","  \n","Construir dos listas de strings que contengan `\"spam\"` o `\"no-spam\"` y que representen la etiqueta real (`y_real`) y la etiqueta predicha por el filtro anti-spam (`y_pred`)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_real = \"...Completar...\"\n","y_pred = \"...Completar...\""]},{"cell_type":"markdown","metadata":{},"source":["Correr la matriz de confusión y verificar que el resultado es el esperado."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred, positive_label=\"spam\", show=False)\n","# Si se cambia show a True se puede visualizar la matriz de confusión\n","\n","print(\"Test 1\")\n","print(\"(tp, tn, fp, fn) = \", (tp, tn, fp, fn))\n","assert((tp, tn, fp, fn) == (2, 958, 20, 10))\n","print(\"OK\")"]},{"cell_type":"markdown","metadata":{},"source":["## Métricas\n","\n","En esta sección trabajeremos con las métricas estándares de clasificación."]},{"cell_type":"markdown","metadata":{},"source":["### Test 2\n","A continuacion completar las funciones que computan las distintas métricas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def accuracy_score(tp: int, tn: int, fp: int, fn: int) -> float:\n","    return \"...Completar...\"\n","\n","\n","def precision_score(tp: int, tn: int, fp: int, fn: int) -> float:\n","    return \"...Completar...\"\n","\n","\n","def recall_score(tp: int, tn: int, fp: int, fn: int) -> float:\n","    return \"...Completar...\"\n","\n","\n","def f_beta_score(tp: int, tn: int, fp: int, fn: int, beta: float) -> float:\n","    prec = precision_score(tp, tn, fp, fn)\n","    recl = recall_score(tp, tn, fp, fn)\n","    return \"...Completar...\"\n","\n","\n","def f1_score(tp: int, tn: int, fp: int, fn: int) -> float:\n","    return f_beta_score(tp, tn, fp, fn, beta=1)\n","\n","\n","def all_metrics(tp: int, tn: int, fp: int, fn: int) -> float:\n","    accuracy = round(accuracy_score(tp, tn, fp, fn), 3)\n","    precision = round(precision_score(tp, tn, fp, fn), 3)\n","    recall = round(recall_score(tp, tn, fp, fn), 3)\n","    f1 = round(f1_score(tp, tn, fp, fn), 3)\n","    return accuracy, precision, recall, f1\n"]},{"cell_type":"markdown","metadata":{},"source":["Evaluar las funciones con el siguiente caso de test."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred, positive_label=\"spam\")\n","(accuracy, precision, recall, f1) = all_metrics(tp, tn, fp, fn)\n","\n","print(\"Test 2\")\n","print(\"(accuracy, precision, recall, f1) = \", (accuracy, precision, recall, f1))\n","assert((accuracy, precision, recall, f1) == (0.97, 0.091, 0.167, 0.118))\n","print(\"OK\")"]},{"cell_type":"markdown","metadata":{},"source":["## Comparando predicciones\n","\n","Sean los siguientes datos provenientes de 2 clasificadores (A y B) y el valor real de las etiquetas."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Etiquetas reales\n","y_real = [\"perro\"] * 18 + [\"gato\"] * 980 + [\"perro\"] * 5\n","\n","# Etiquetas devueltas por \"clasificador A\"\n","y_pred_A = [\"gato\"] * 980 + [\"perro\"] * 20 + [\"gato\"] * 3\n","\n","# Etiquetas devueltas por \"clasificador B\"\n","y_pred_B = [\"perro\"] * 40 + [\"gato\"] * 900 + [\"perro\"] * 60 + [\"gato\"] * 3\n","\n","df = pd.DataFrame(data={\"y_real\": y_real,\n","                           \"y_pred_A\": y_pred_A,\n","                           \"y_pred_B\": y_pred_B,                           \n","                          })\n","\n","res = []\n","print(\"Clasificador A, etiqueta de éxito: gato\")\n","tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_A, positive_label=\"gato\", show=True)\n","res.append(all_metrics(tp, tn, fp, fn))\n","\n","print(\"Clasificador B, etiqueta de éxito: gato\")\n","tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_B, positive_label=\"gato\", show=True)\n","res.append(all_metrics(tp, tn, fp, fn))\n","\n","print(\"Clasificador A, etiqueta de éxito: perro\")\n","tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_A, positive_label=\"perro\", show=True)\n","res.append(all_metrics(tp, tn, fp, fn))\n","\n","print(\"Clasificador B, etiqueta de éxito: perro\")\n","tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_B, positive_label=\"perro\", show=True)\n","res.append(all_metrics(tp, tn, fp, fn))\n","\n","pd.DataFrame(res, columns=[\"accuracy\", \"precision\", \"recall\", \"f1\"], index=[\"CLF A (gato)\", \"CLF B (gato)\", \"CLF A (perro)\", \"CLF B (gato)\"])"]},{"cell_type":"markdown","metadata":{},"source":["¿Qué podemos concluir con este experimento?\n"]},{"cell_type":"markdown","metadata":{},"source":["## Analizando $F_1$\n","\n","A continuación realizamos un experimento variando levemente las condiciones en cada pasada.\n","\n","El código que realiza el experimento es el siguiente:"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["y_real = [\"perro\"] * 100 + [\"gato\"] * 900 + [\"perro\"] * 80\n","y_pred =   [\"perro\"] * 80 + [\"gato\"] * 800 + [\"perro\"] * 200\n","\n","tns_gato = []\n","f1s_gato = []\n","f1s_perro = []\n","f1s_avg = []\n","\n","\n","for i in range(0, 10000, 100):\n","    y_real_2 = y_real + [\"perro\"] * i\n","    y_pred_2 = y_pred + [\"perro\"] * i\n","\n","    tp1, tn1, fp1, fn1 = confusion_matrix(y_real=y_real_2, y_predicted=y_pred_2, positive_label=\"gato\")\n","    tp2, tn2, fp2, fn2 = confusion_matrix(y_real=y_real_2, y_predicted=y_pred_2, positive_label=\"perro\")\n","\n","    f1_gato = f1_score(tp1, tn1, fp1, fn1)\n","    f1_perro = f1_score(tp2, tn2, fp2, fn2)\n","    f1_avg = (f1_gato + f1_perro) / 2\n","\n","    tns_gato.append(tn1)\n","    f1s_gato.append(f1_gato)\n","    f1s_perro.append(f1_perro)\n","    f1s_avg.append(f1_avg)"]},{"cell_type":"markdown","metadata":{},"source":["1. ¿Qué realiza este experimento?\n","1. ¿Qué relación existe entre la $F_1$ de perro y de gato a medida que se aumenta la cantidad de perros que tiene la muestra?\n","1. ¿En algún punto valen lo mismo?¿En cuál?¿Por qué?"]},{"cell_type":"markdown","metadata":{},"source":["A continuación se propone graficar cómo varía la métrica $F_1$ al aumentar la cantidad de True Negatives (observar\n","que estamos cambiando la cantidad de instancias sobre las que testeamos). \n","\n","1. ¿Qué curva modifica más el agregado de las etiquetas `perro`?¿Por qué? \n","1. ¿Qué se puede concluir de este experimento?\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(15, 10))\n","\n","plt.plot(tns_gato, f1s_gato, \"*-\", label=\"F1_gato\")\n","plt.plot(tns_gato, f1s_perro, \"o-\", label=\"F1_perro\")\n","plt.plot(tns_gato, f1s_avg, \"x-\", label=\"F1_avg\")\n","plt.xlabel(\"True Negatives (Gato)\")\n","plt.ylabel(\"F1 score\")\n","plt.ylim([0.5,1])\n","plt.legend()\n","plt.show()\n"]}],"metadata":{"celltoolbar":"Tags","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":2}
